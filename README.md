# ğŸ§¬ Hybrid DNA Variant Distance Regression
Reference vs Variant DNA ì„ë² ë”© ê±°ë¦¬ í•™ìŠµì„ í†µí•œ ë³€ì´ ë³‘ì›ì„± ëª¨ë¸ë§

<br>

ì´ ì €ì¥ì†ŒëŠ” **DNA ë³€ì´ ë³‘ì›ì„±(Pathogenicity) í•™ìŠµì„ ìœ„í•œ ì½”ë“œë§Œ** í¬í•¨í•©ë‹ˆë‹¤.  
ìœ ì „ì²´ ë°ì´í„°, ì‚¬ì „í•™ìŠµ ëª¨ë¸ ê°€ì¤‘ì¹˜, íŒŒì¸íŠœë‹ ì²´í¬í¬ì¸íŠ¸ ë° ì‹¤í—˜ ë¡œê·¸ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

- ì…ë ¥: Reference DNA ì„œì—´ / Variant DNA ì„œì—´  
- ì¶œë ¥: L2 ì •ê·œí™” ì„ë² ë”©(2048-d) ë° Cosine Distance âˆˆ [0, 2]

<br>

---

## ğŸš¦ ëª©ì°¨

1. [ğŸ”¬ í”„ë¡œì íŠ¸ ì†Œê°œ](#intro)  
   1-1. [ğŸ‘¥ íŒ€ì› ì†Œê°œ](#team)  
2. [ğŸ§© ë¬¸ì œ ì •ì˜](#problem)  
3. [ğŸ§  ë°©ë²•ë¡  ìš”ì•½](#method)  
4. [ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°](#model)  
5. [ğŸ“ í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜](#loss)  
6. [ğŸ“Š í‰ê°€ ì§€í‘œ](#metrics)  
7. [ğŸ“ ì €ì¥ì†Œ êµ¬ì¡°](#structure)  
8. [âš™ï¸ ì„¤ì¹˜ ë° ë¹ ë¥¸ ì‹œì‘](#quickstart)  
   8-1. [ğŸ§° í™˜ê²½ êµ¬ì„±](#install)  
   8-2. [ğŸ—‚ï¸ ë°ì´í„° ì¤€ë¹„](#data)  
   8-3. [ğŸ§¹ ì „ì²˜ë¦¬](#preprocess)  
   8-4. [ğŸ‹ï¸ í•™ìŠµ](#train)  
   8-5. [ğŸ” ì¶”ë¡ ](#inference)  
9. [ğŸ§ª ë¡œê¹… ë° ì¬í˜„ì„±](#repro)  
10. [ğŸš« ë°ì´í„°/ëª¨ë¸ ì¬ë°°í¬ ì•ˆë‚´](#policy)  
11. [ğŸ› ï¸ FAQ / íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#faq)  
12. [ğŸ“¬ ë¬¸ì˜](#contact)  

<br><br><br>

---

<a id="intro"></a>
# 1. ğŸ”¬ í”„ë¡œì íŠ¸ ì†Œê°œ

### í”„ë¡œì íŠ¸ê°€ í•˜ëŠ” ì¼

ë³¸ í”„ë¡œì íŠ¸ëŠ” **Reference DNA ì„œì—´ê³¼ Variant DNA ì„œì—´**ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê°ê° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³ ,  
**ë‘ ì„ë² ë”©ì˜ ê±°ë¦¬(distance)** ê°€ ë³€ì´ ë³‘ì›ì„± ì‹ í˜¸ì™€ ì¼ê´€ë˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.

ì¦‰, ë³€ì´ ë³‘ì›ì„±ì„ â€œë¶„ë¥˜(label)â€ë¡œë§Œ ë§íˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì„ë² ë”© ê³µê°„ì—ì„œ:

- Pathogenic ìƒ˜í”Œë¼ë¦¬ëŠ” ë” ê°€ê¹ê²Œ
- Benign ìƒ˜í”Œê³¼ëŠ” ë” ë©€ê²Œ
- ë³‘ì›ì„± ì ìˆ˜(ë˜ëŠ” ì—°ì† íƒ€ê¹ƒ)ì™€ ê±°ë¦¬ê°€ ì •í•©ë˜ê²Œ

ì •ë ¬ë˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.

### ì´ ì ‘ê·¼ì´ ìœ ìš©í•œ ê²½ìš°

- ë³€ì´ íš¨ê³¼ë¥¼ ë‹¨ì¼ ì ìˆ˜ ì˜ˆì¸¡ë³´ë‹¤ **ê±°ë¦¬/ìœ ì‚¬ë„ ê¸°ë°˜ í‘œí˜„**ìœ¼ë¡œ ë‹¤ë£¨ê³  ì‹¶ì„ ë•Œ
- downstream(ê²€ìƒ‰, í´ëŸ¬ìŠ¤í„°ë§, ìœ ì‚¬ ë³€ì´ íƒìƒ‰, metric learning)ê¹Œì§€ ê³ ë ¤í•œ ì„ë² ë”©ì´ í•„ìš”í•  ë•Œ
- ì „ì—­ ë¬¸ë§¥(ìœ ì „ì²´ ì–¸ì–´ëª¨ë¸)ê³¼ êµ­ì†Œ ë³€ì´ ì‹ í˜¸(ë³€ì´ ì£¼ë³€) ëª¨ë‘ë¥¼ ë°˜ì˜í•˜ê³  ì‹¶ì„ ë•Œ

### ì „ì²´ íŒŒì´í”„ë¼ì¸ ê°œìš”

```text
ClinVar (VCF) + Reference Genome (hg38 FASTA)
            â”‚
            â–¼
   src/data_preprocess.py
            â”‚  (reference/variant pair + label/score ìƒì„±)
            â–¼
        src/train.py
            â”‚  (LoRA fine-tuning + multi-loss distance learning)
            â–¼
     fine-tuned checkpoint
            â”‚
            â–¼
      src/inference.py
            â”‚  (embedding export / distance compute)
            â–¼
  Embeddings (L2-normalized, 2048-d) + Cosine Distance
```

<br>

<a id="team"></a>
## 1-1. ğŸ‘¥ íŒ€ì› ì†Œê°œ

<table>
  <tr>
    <td align="center" width="220">
      <img src="https://placehold.co/200x200?text=%EC%95%88%EC%A4%80%EC%8B%9D" width="200" height="200" alt="ì•ˆì¤€ì‹"><br>
      ì•ˆì¤€ì‹<br><sub>Lead Researcher</sub>
    </td>
    <td align="center" width="220">
      <img src="https://placehold.co/200x200?text=%EC%9C%A4%EC%97%AC%ED%97%8C" width="200" height="200" alt="ìœ¤ì—¬í—Œ"><br>
      ìœ¤ì—¬í—Œ<br><sub>Bio-ML Engineer</sub>
    </td>
    <td align="center" width="220">
      <img src="https://avatars.githubusercontent.com/u/150754838?v=4" width="200" height="200" alt="ì¥ì˜ì›…"><br>
      ì¥ì˜ì›…<br><sub>Read Model Engineer</sub>
    </td>
    <td align="center" width="220">
      <img src="https://placehold.co/200x200?text=%EC%9D%B4%EC%A0%95%EC%9B%90" width="200" height="200" alt="ì´ì •ì›"><br>
      ì´ì •ì›<br><sub>Data Engineer</sub>
    </td>
    <td align="center" width="220">
      <img src="https://placehold.co/200x200?text=%EC%A1%B0%EB%AF%BC%EC%84%B1" width="200" height="200" alt="ì¡°ë¯¼ì„±"><br>
      ì¡°ë¯¼ì„±<br><sub>Data Researcher</sub>
    </td>
  </tr>
  <tr>
    <td align="center" width="220">
      ì—°êµ¬ ì´ê´„, ë¬¸ì œ ì •ì˜, ì „ì²´ ëª¨ë¸ ì•„í‚¤í…ì²˜ ë°©í–¥ ì„¤ê³„
    </td>
    <td align="center" width="220">
      ì˜ìƒëª… ì„œì—´ ë°ì´í„° ê¸°ë°˜<br>í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬í˜„,<br>ë³€ì´Â·ì„œì—´ íŠ¹ì„± ì„±ëŠ¥ ë¶„ì„
    </td>
    <td align="center" width="220">
      í•™ìŠµ íŒŒì´í”„ë¼ì¸ ìµœì í™”,<br>ì§€í‘œ ì •í•©ì„± ê°œì„ , íŒŒì¸íŠœë‹<br>ê¸°ë°˜ ì„±ëŠ¥ í–¥ìƒÂ·ì•ˆì •í™”
    </td>
    <td align="center" width="220">
      ë°ì´í„° ì „ì²˜ë¦¬, ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
    </td>
    <td align="center" width="220">
      ClinVar ê¸°ë°˜ ë³€ì´ ë¶„ì„, ë³‘ì›ì„± íŠ¹ì„± ì—°êµ¬
    </td>
  </tr>
</table>


---

<a id="problem"></a>
# 2. ğŸ§© ë¬¸ì œ ì •ì˜

- ì…ë ¥: Reference DNA ì„œì—´ / Variant DNA ì„œì—´ (ì„œì—´ ìŒ)
- ì¶œë ¥: L2 ì •ê·œí™” ì„ë² ë”© ë²¡í„°(2048-d)
- ê±°ë¦¬: L2 ì •ê·œí™”ëœ ë‘ ì„ë² ë”©ì˜ **cosine distance** ì‚¬ìš© (ë²”ìœ„ [0, 2])

í•™ìŠµ ëª©ì ì€ â€œì •ë‹µ ë¼ë²¨ì„ ì§ì ‘ ë§íˆê¸°â€ê°€ ì•„ë‹ˆë¼,
**ì„ë² ë”© ê³µê°„ì˜ ê±°ë¦¬ êµ¬ì¡°**ê°€ ë³‘ì›ì„± ì‹ í˜¸ì™€ ì •ë ¬ë˜ë„ë¡ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

---

<a id="method"></a>
# 3. ğŸ§  ë°©ë²•ë¡  ìš”ì•½

í•µì‹¬ì€ **ì „ì—­(Genomic LM) + êµ­ì†Œ(Local CNN)** í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”©ì…ë‹ˆë‹¤.

- Global(ì „ì—­): Genomic Language Modelë¡œ ì„œì—´ ì „ì—­ ë¬¸ë§¥ì„ ì¸ì½”ë”©
- Local(êµ­ì†Œ): ë³€ì´ ìœ„ì¹˜ ì£¼ë³€(one-hot) ì…ë ¥ì„ 1D CNNìœ¼ë¡œ ì¸ì½”ë”©í•´ ë³€ì´ ë¯¼ê°ë„ ê°•í™”
- Fusion: Global + Local ê²°í•© â†’ Linear Projection â†’ L2 ì •ê·œí™”
- Training: ë‹¤ì¤‘ ì†ì‹¤ì„ ê°€ì¤‘í•©ìœ¼ë¡œ ê²°í•©í•´ ì„ë² ë”© êµ¬ì¡°ë¥¼ ì•ˆì •í™”

---

<a id="model"></a>
# 4. ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°

### Backbone (Global Encoder)

- Hugging Face ê³µê°œ ëª¨ë¸  
  `LongSafari/hyenadna-large-1m-seqlen-hf`
- íŒŒì¸íŠœë‹ ë°©ì‹  
  LoRA (Parameter-Efficient Fine-Tuning)

### Local Encoder

- ë³€ì´ ìœ„ì¹˜ ê¸°ì¤€ one-hot DNA ì„œì—´
- 1D CNN ê¸°ë°˜ êµ­ì†Œ íŠ¹ì§• ê°•í™”

### Final Embedding

- Global + Local ê²°í•©
- Linear projection í›„ L2 normalization
- Embedding dimension: 2048

### ìˆ˜í•™ì  ì •ì˜ (Notation)

- Reference/Variant ì„œì—´: $x_{ref}, x_{var} \in \{A,C,G,T,N\}^{L}$, $L=1024$
- ë³€ì´ ìƒëŒ€ ìœ„ì¹˜: $m$ (ì»¬ëŸ¼ `mut_index`)
- ë¼ë²¨/ì ìˆ˜: $y\in\{0,1\}$, $s\in\{0.0,0.2,0.8,1.0\}$
- ëª©í‘œ ê±°ë¦¬(ê°ë… ì‹ í˜¸): $d^{\ast}=2s\in[0,2]$

### Global Encoder (HyenaDNA backbone)

ë°±ë³¸ì˜ ë§ˆì§€ë§‰ hidden stateë¥¼ $H\in\mathbb{R}^{L\times d}$ë¼ í•˜ë©´,

- mean pooling: $\bar{h}=\frac{1}{L}\sum_{i=1}^{L}H_i\in\mathbb{R}^{d}$
- projection: $g=W_g\bar{h}+b_g\in\mathbb{R}^{1024}$

### Local Encoder (Mutation-centered window + 1D CNN)

ë³€ì´ ìœ„ì¹˜ $m$ì„ ê¸°ì¤€ìœ¼ë¡œ local window ê¸¸ì´ $w=64$ë¥¼ ì¶”ì¶œí•´ one-hotìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

- one-hot: $X_{local}\in\{0,1\}^{4\times w}$
- CNN ì¶œë ¥: $\ell=\mathrm{CNN}(X_{local})\in\mathbb{R}^{1024}$

### Fusion + Final Embedding

- concat: $c=[g;\ell]\in\mathbb{R}^{2048}$
- final projection: $\tilde{z}=W_f c+b_f\in\mathbb{R}^{2048}$
- L2 normalize: $z=\frac{\tilde{z}}{\lVert\tilde{z}\rVert_2}$

### Distance (Cosine Distance)

- cosine similarity: $\cos(z_a,z_b)=\frac{z_a^\top z_b}{\lVert z_a\rVert_2\,\lVert z_b\rVert_2}$
- cosine distance (final): $\hat{d}=1-\cos(z_{ref},z_{var})\in[0,2]$

Mutation Focus Lossìš© local distanceëŠ” $\ell$ì„ ì •ê·œí™” í›„ ë™ì¼í•˜ê²Œ ê³„ì‚°í•©ë‹ˆë‹¤.

- $`\hat{d}_{\mathrm{local}} = 1 - \cos\big(\mathrm{norm}(\ell_{\mathrm{ref}}),\ \mathrm{norm}(\ell_{\mathrm{var}})\big)`$


---

<a id="loss"></a>
# 5. ğŸ“ í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜ (Loss Functions)

ë³¸ ëª¨ë¸ì€ ì•„ë˜ ì†ì‹¤ë“¤ì˜ **ê°€ì¤‘í•©(weighted sum)** ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

- Distance Regression Loss (MSE)  
  ì˜ˆì¸¡ ê±°ë¦¬ì™€ ë³‘ì›ì„± ì ìˆ˜(ë˜ëŠ” íƒ€ê¹ƒ ê±°ë¦¬) ì •í•©
- Mutation Focus Loss  
  ë³€ì´ ì£¼ë³€ êµ­ì†Œ íŠ¹ì§• ë¯¼ê°ë„ ê°•í™”
- Mean Margin Gap Loss  
  Pathogenic/Benign í‰ê·  ê±°ë¦¬ ì°¨ì´ í™•ë³´
- Triplet Loss  
  ì„ë² ë”© êµ¬ì¡° ë¶„ë¦¬ ê°•í™”
- Pairwise Margin Loss  
  í´ë˜ìŠ¤ ê°„ ë§ˆì§„ ìœ ì§€ ê°•í™”
- Supervised Contrastive Loss  
  ì„ë² ë”©ë§Œ ì‚¬ìš©í•´ë„ í´ë˜ìŠ¤ ë¶„ë¦¬ê°€ ìœ ì§€ë˜ë„ë¡ ì •ë ¬

ì†ì‹¤ì˜ ìƒì„¸ ì •ì˜/ê°€ì¤‘ì¹˜ëŠ” `Train.md` ë° `src/train.py` ì„¤ì •(CONFIG ë“±)ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤.

### ì†ì‹¤ í•¨ìˆ˜ ìˆ˜í•™ì  ì •ì˜

ëª¨ë“  $\hat{d}$ëŠ” (Reference, Variant) ìµœì¢… ì„ë² ë”©ì˜ cosine distanceì…ë‹ˆë‹¤.

- ìµœì¢… ê±°ë¦¬: $\hat{d}=1-\cos(z_{ref},z_{var})$
local ê±°ë¦¬: $`\hat{d}_{\mathrm{local}} = 1 - \cos\big(\mathrm{norm}(\ell_{\mathrm{ref}}),\ \mathrm{norm}(\ell_{\mathrm{var}})\big)`$
- íƒ€ê¹ƒ ê±°ë¦¬: $d^{\ast}=2s$  (ì½”ë“œì—ì„œ `target_dist = score * 2.0`)

ë°°ì¹˜ì—ì„œ Pathogenic ì§‘í•© $P=\{i\mid y_i=1\}$, Benign ì§‘í•© $B=\{i\mid y_i=0\}$ë¡œ ë‘ë©´,

1) Distance Regression Loss (MSE)

```math
\mathcal{L}_{reg}=\frac{1}{N}\sum_{i=1}^{N}(\hat{d}_i-d^{\ast}_i)^2
```
2) Mutation Focus Loss (Local CNN MSE)

```math
\mathcal{L}_{focus}=\frac{1}{N}\sum_{i=1}^{N}(\hat{d}_{local,i}-d^{\ast}_i)^2
```
3) Mean Margin Gap Loss (Path vs Benign í‰ê·  ê±°ë¦¬ ì°¨ì´)

```math
\mu_P=\frac{1}{\lvert P\rvert}\sum_{i\in P}\hat{d}_i,\quad
\mu_B=\frac{1}{\lvert B\rvert}\sum_{j\in B}\hat{d}_j,\quad
\mathcal{L}_{mean}=\max(0, m_{mean}-(\mu_P-\mu_B))
```
ì—¬ê¸°ì„œ $m_{mean}$ì€ `CONFIG["margin_value"]` ì…ë‹ˆë‹¤.

4) Batch-level Triplet (Path intra vs Pathâ€“Benign)
Pathogenic anchor $i\in P$ì— ëŒ€í•´,

```math
\text{pos\_mean}_i=\frac{1}{\lvert P\rvert-1}\sum_{j\in P, j\neq i}(1-\cos(z_i,z_j)),\quad
\text{neg\_mean}_i=\frac{1}{\lvert B\rvert}\sum_{k\in B}(1-\cos(z_i,z_k))
```

```math
\mathcal{L}_{triplet}=\frac{1}{\lvert P\rvert}\sum_{i\in P}\max(0,\text{pos\_mean}_i-\text{neg\_mean}_i+m_{triplet})
```
ì—¬ê¸°ì„œ $m_{triplet}$ì€ `CONFIG["triplet_margin"]` ì…ë‹ˆë‹¤.

5) Pairwise Margin Loss (Pathâ€“Benign ìŒë³„ ë§ˆì§„)

```math
\mathcal{L}_{pair}=\frac{1}{\lvert P\rvert\,\lvert B\rvert}\sum_{i\in P}\sum_{j\in B}\max(0, m_{pair}-(\hat{d}_i-\hat{d}_j))
```
ì—¬ê¸°ì„œ $m_{pair}$ëŠ” `CONFIG["pair_margin_value"]` ì…ë‹ˆë‹¤.

6) Supervised Contrastive Loss
ì •ê·œí™” ì„ë² ë”© $z_i$ì— ëŒ€í•´, $A(i)=\{a\neq i\}$, $P(i)=\{p\neq i\mid y_p=y_i\}$.

```math
\mathcal{L}_{supcon}
=
-\frac{1}{|\{i:|P(i)|>0\}|}
\sum_{i:|P(i)|>0}
\frac{1}{|P(i)|}
\sum_{p\in P(i)}
\log
\frac{\exp(z_i^\top z_p/\tau)}{\sum_{a\in A(i)}\exp(z_i^\top z_a/\tau)}
```
ì—¬ê¸°ì„œ $\tau$ëŠ” `CONFIG["contrast_temperature"]` ì…ë‹ˆë‹¤.

7) Total loss (ê°€ì¤‘í•©)

```math
\mathcal{L}
=
\text{loss\_scale}\cdot(
w_{reg}\mathcal{L}_{reg}
+w_{focus}\mathcal{L}_{focus}
+w_{mean}\mathcal{L}_{mean}
+w_{triplet}\mathcal{L}_{triplet}
+w_{pair}\mathcal{L}_{pair}
+w_{supcon}\mathcal{L}_{supcon}
)
```
ê° ê°€ì¤‘ì¹˜ëŠ” ì½”ë“œì˜ `CONFIG["w_*"]` í•­ëª©ì„ ë”°ë¦…ë‹ˆë‹¤.


---

<a id="metrics"></a>
# 6. ğŸ“Š í‰ê°€ ì§€í‘œ (Metrics)

í•™ìŠµ ë° í‰ê°€ì—ì„œ ë‹¤ìŒ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

- CD (Cosine Distance Mean)
- CDD (Class Distance Difference)
- PCC (Pearson Correlation Coefficient)
- ì •ê·œí™” ì§€í‘œ(0~1 ìŠ¤ì¼€ì¼)
- Final Score: Normalized CD, CDD, PCCì˜ í‰ê· 

### ì§€í‘œ ìˆ˜í•™ì  ì •ì˜

ê±°ë¦¬ ë°°ì—´ $`D=\{\hat{d}_i\}_{i=1}^{N}`$ì— ëŒ€í•´,

- CD (Cosine Distance Mean)

```math
\mathrm{CD}=\frac{1}{N}\sum_{i=1}^{N}\hat{d}_i
```
- CDD (Class Distance Difference)

```math
\mu_P=\frac{1}{\lvert P\rvert}\sum_{i\in P}\hat{d}_i,\quad
\mu_B=\frac{1}{\lvert B\rvert}\sum_{j\in B}\hat{d}_j,\quad
\mathrm{CDD}=\frac{\mu_P-\mu_B}{2}
```
(ì½”ë“œ ê¸°ì¤€ìœ¼ë¡œ $\mu_P-\mu_B$ë¥¼ 2ë¡œ ë‚˜ëˆ  ìŠ¤ì¼€ì¼ì„ ë§ì¶¥ë‹ˆë‹¤)

- PCC (Pearson Correlation)

```math
\mathrm{PCC}=\rho(y,\hat{d})
```
- Normalization (0~1 clamp)

```math
\mathrm{Normal\_CD}=\mathrm{clip}(\mathrm{CD}/2,0,1)
```

```math
\mathrm{Normal\_CDD}=\mathrm{clip}((\mathrm{CDD}+1)/2,0,1)
```

```math
\mathrm{Normal\_PCC}=\mathrm{clip}((\mathrm{PCC}+1)/2,0,1)
```
- Final Score

```math
\mathrm{FinalScore}=\frac{\mathrm{Normal\_CD}+\mathrm{Normal\_CDD}+\mathrm{Normal\_PCC}}{3}
```
---

<a id="structure"></a>
# 7. ğŸ“ ì €ì¥ì†Œ êµ¬ì¡°

```text
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py               # ëª¨ë¸ í•™ìŠµ ì½”ë“œ
â”‚   â”œâ”€â”€ data_preprocess.py     # ë°ì´í„°ì…‹ ìƒì„± ì½”ë“œ (ClinVar ê¸°ë°˜)
â”‚   â””â”€â”€ inference.py           # ì¶”ë¡  ë° ì„ë² ë”© ìƒì„± ì½”ë“œ
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ Data_Preprocessing.md
â”œâ”€â”€ Train.md
â”œâ”€â”€ Inference.md
â””â”€â”€ .gitignore
```

---

<a id="quickstart"></a>
# 8. âš™ï¸ ì„¤ì¹˜ ë° ë¹ ë¥¸ ì‹œì‘

ì°¸ê³ 
- ë³¸ ì €ì¥ì†Œì˜ `src/*.py`ëŠ” argparse ê¸°ë°˜ CLI ì¸ìë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì•„ë˜ì˜ "1) CLI ì¸ìë¥¼ ì§€ì›í•˜ëŠ” ê²½ìš°" ì˜ˆì‹œëŠ” ë¬¸ì„œ í…œí”Œë¦¿ì´ë©°, ì‹¤ì œ ì‹¤í–‰ì€ "2) CLIê°€ ì—†ë‹¤ë©´" ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•˜ì„¸ìš”.



<a id="install"></a>
## 8-1. ğŸ§° í™˜ê²½ êµ¬ì„± (Conda ê¶Œì¥)

```bash
conda create -n medicalAI python=3.10
conda activate medicalAI
pip install -r requirements.txt
```

ì°¸ê³ 
- CUDA/cuDNN/ë“œë¼ì´ë²„ëŠ” ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ë³„ë„ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
- CPUì—ì„œë„ ì‹¤í–‰ì€ ê°€ëŠ¥í•˜ì§€ë§Œ í•™ìŠµ ì†ë„ëŠ” í¬ê²Œ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê¸´ ì‹œí€€ìŠ¤/ëŒ€í˜• ë°±ë³¸ì€ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì´ í½ë‹ˆë‹¤. ë¨¼ì € ì‘ì€ ì„¤ì •ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ì„ ê²€ì¦í•˜ì„¸ìš”.

<br>

<a id="data"></a>
## 8-2. ğŸ—‚ï¸ ë°ì´í„° ì¤€ë¹„

ì´ ì €ì¥ì†ŒëŠ” ë°ì´í„° íŒŒì¼ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ ê³µì‹ ì¶œì²˜ì—ì„œ ì§ì ‘ íšë“í•´ì•¼ í•©ë‹ˆë‹¤.

- Reference Genome: UCSC Genome Browser (hg38)
- Variant Annotation: ClinVar (NCBI)

ê¶Œì¥ ë””ë ‰í„°ë¦¬ ì˜ˆì‹œ

```text
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ hg38.fa
â”‚   â””â”€â”€ clinvar.vcf.gz
â””â”€â”€ processed/
    â”œâ”€â”€ train.csv
    â”œâ”€â”€ valid.csv
    â””â”€â”€ test.csv
```

<br>

<a id="preprocess"></a>
## 8-3. ğŸ§¹ ì „ì²˜ë¦¬ (Data Preprocessing)

ì „ì²˜ë¦¬ ëª©í‘œ

- ClinVar ë³€ì´ ì •ë³´(VCF) íŒŒì‹±
- ë³€ì´ ì¢Œí‘œ ê¸°ë°˜ reference/variant ì„œì—´ ìƒì„±
- í•™ìŠµìš© pair ë°ì´í„°ì…‹(ì˜ˆ: CSV) ìƒì„±
- variant ë‹¨ìœ„ splitë¡œ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€

ì‹¤í–‰ ë°©ì‹ì€ ì•„ë˜ ë‘ íŒ¨í„´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

1) CLI ì¸ìë¥¼ ì§€ì›í•˜ëŠ” ê²½ìš°

```bash
python src/data_preprocess.py -h
python src/data_preprocess.py --input_vcf <VCF_PATH> --ref_fa <FASTA_PATH> --out_dir <OUT_DIR>
```

2) CLIê°€ ì—†ë‹¤ë©´  
`src/data_preprocess.py` ìƒë‹¨ì˜ ì„¤ì •(CONFIG/ê²½ë¡œ)ì„ ìˆ˜ì • í›„ ì‹¤í–‰

```bash
python src/data_preprocess.py
```

ì „ì²˜ë¦¬ ì…ë ¥/ì¶œë ¥ ìŠ¤í‚¤ë§ˆ(ì»¬ëŸ¼ëª…/í¬ë§·)ëŠ” `Data_Preprocessing.md`ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶”ì„¸ìš”.

<br>

<a id="train"></a>
## 8-4. ğŸ‹ï¸ í•™ìŠµ (Training)

í•™ìŠµ ëª©í‘œ

- í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© ëª¨ë¸ êµ¬ì„±(Global LM + Local CNN)
- cosine distance ê¸°ë°˜ ë³‘ì›ì„± ì‹ í˜¸ í•™ìŠµ
- multi-lossë¡œ ì„ë² ë”© êµ¬ì¡° ì•ˆì •í™”

ì‹¤í–‰ ë°©ì‹ì€ ì „ì²˜ë¦¬ì™€ ë™ì¼í•˜ê²Œ ë‘ íŒ¨í„´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

1) CLI ì¸ìë¥¼ ì§€ì›í•˜ëŠ” ê²½ìš°

```bash
python src/train.py -h
python src/train.py --data_dir <PROCESSED_DIR> --output_dir <OUT_DIR>
```

2) CLIê°€ ì—†ë‹¤ë©´  
`src/train.py` ë‚´ë¶€ ì„¤ì •(CONFIG ë“±)ì—ì„œ ë‹¤ìŒì„ í™•ì¸ í›„ ì‹¤í–‰

- ë°ì´í„° ê²½ë¡œ(`data/processed/<...>`)
- ë°±ë³¸/LoRA ì„¤ì •
- Local encoder ì„¤ì •(ìœˆë„ìš° í¬ê¸°, CNN ì±„ë„/ì»¤ë„ ë“±)
- ì†ì‹¤ ê°€ì¤‘ì¹˜
- í•˜ì´í¼íŒŒë¼ë¯¸í„°(lr, batch size, epochs, grad accumulation ë“±)
- ì²´í¬í¬ì¸íŠ¸/ë¡œê·¸ ì €ì¥ ê²½ë¡œ

```bash
python src/train.py
```

í•™ìŠµ ìƒì„¸ ì˜µì…˜ê³¼ ì¬í˜„ ê·œì¹™ì€ `Train.md`ë¥¼ ìš°ì„  í™•ì¸í•˜ì„¸ìš”.

<br>

<a id="inference"></a>
## 8-5. ğŸ” ì¶”ë¡  (Inference / Embedding Export)

ì¶”ë¡  ëª©í‘œ

- fine-tuned checkpoint ë¡œë“œ
- embedding ìƒì„± ë° ì €ì¥
- í•„ìš” ì‹œ cosine distance ì‚°ì¶œ

ì‹¤í–‰ ë°©ì‹ì€ ì•„ë˜ ë‘ íŒ¨í„´ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

1) CLI ì¸ìë¥¼ ì§€ì›í•˜ëŠ” ê²½ìš°

```bash
python src/inference.py -h
python src/inference.py --ckpt <CKPT_PATH> --input <CSV_PATH> --out <OUT_PATH>
```

2) CLIê°€ ì—†ë‹¤ë©´  
`src/inference.py` ë‚´ë¶€ ì„¤ì •(CONFIG/ê²½ë¡œ)ì„ ìˆ˜ì • í›„ ì‹¤í–‰

```bash
python src/inference.py
```

ì¶”ë¡  ì…ì¶œë ¥(íŒŒì¼ í¬ë§·/ì €ì¥ ë°©ì‹)ì€ `Inference.md`ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶”ì„¸ìš”.

---

<a id="repro"></a>
# 9. ğŸ§ª ë¡œê¹… ë° ì¬í˜„ì„± (Logging & Reproducibility)

### Weights & Biases (wandb)

- í•™ìŠµ/í‰ê°€ ë¡œê·¸ ê¸°ë¡ì— wandbë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° `src/train.py`ì˜ `wandb.init()`, `wandb.log()`ë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.

### ì¬í˜„ì„±

- Python/NumPy/PyTorch ì‹œë“œ ê³ ì •
- ì„¤ì •ì€ CONFIG(ë˜ëŠ” ìœ ì‚¬ ì„¤ì • ë¸”ë¡)ë¡œ ì¤‘ì•™ ê´€ë¦¬
- variant ë‹¨ìœ„ splitë¡œ ëˆ„ìˆ˜ ë°©ì§€

---

<a id="policy"></a>
# 10. ğŸš« ë°ì´í„° ë° ëª¨ë¸ ì¬ë°°í¬ ì•ˆë‚´ (ì¤‘ìš”)

ë³¸ ì €ì¥ì†ŒëŠ” **ì—°êµ¬ ë° êµìœ¡ ëª©ì ì˜ ì½”ë“œë§Œ ê³µê°œ**í•©ë‹ˆë‹¤. ì•„ë˜ í•­ëª©ì€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©° ì¬ë°°í¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

- ì¸ê°„ ìœ ì „ì²´ Reference Genome (ì˜ˆ: hg38)
- ClinVar ì›ë³¸ VCF
- ClinVar ê¸°ë°˜ íŒŒìƒ ë°ì´í„°ì…‹(CSV ë“±)
- ì‚¬ì „í•™ìŠµ(pretrained) ëª¨ë¸ ê°€ì¤‘ì¹˜
- íŒŒì¸íŠœë‹(fine-tuned) ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
- í•™ìŠµ ê²°ê³¼ë¬¼ ë° ì‹¤í—˜ ë¡œê·¸

ì‚¬ì „í•™ìŠµ ëª¨ë¸ì€ Hugging Face Hubì—ì„œ ë™ì ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.  
ì‚¬ìš© ì‹œ ê° ë°ì´í„°/ëª¨ë¸ì˜ ë¼ì´ì„ ìŠ¤ ë° ì´ìš© ì •ì±…ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

---

<a id="faq"></a>
# 11. ğŸ› ï¸ FAQ / íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Q1. ì‹¤í–‰ì´ ëŠë¦¬ê±°ë‚˜ OOMì´ ë‚©ë‹ˆë‹¤
- batch sizeë¥¼ ì¤„ì´ì„¸ìš”.
- ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¤„ì´ê±°ë‚˜(local window í¬í•¨) gradient accumulationì„ ì‚¬ìš©í•˜ì„¸ìš”.
- mixed precision ì‚¬ìš© ì—¬ë¶€(ì§€ì› ì‹œ)ë¥¼ í™•ì¸í•˜ì„¸ìš”.
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í° ë°±ë³¸ì´ë¯€ë¡œ ì‘ì€ ì„¤ì •ìœ¼ë¡œ end-to-end ê²€ì¦ í›„ ìŠ¤ì¼€ì¼ì—…ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

### Q2. ì „ì²˜ë¦¬ ê²°ê³¼ íŒŒì¼ í˜•ì‹ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤
- `Data_Preprocessing.md`ì˜ ì¶œë ¥ ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì»¬ëŸ¼/í¬ë§·ì„ ë§ì¶”ì„¸ìš”.
- CLI ì˜µì…˜ì´ ì—†ë‹¤ë©´ `src/data_preprocess.py` ë‚´ë¶€ ì„¤ì •(CONFIG)ì„ ìš°ì„  í™•ì¸í•˜ì„¸ìš”.

### Q3. í•™ìŠµ ì²´í¬í¬ì¸íŠ¸/ì¶œë ¥ ê²½ë¡œë¥¼ ë°”ê¾¸ê³  ì‹¶ìŠµë‹ˆë‹¤
- `src/train.py`ì˜ output_dir(ë˜ëŠ” CONFIG) í•­ëª©ì„ ìˆ˜ì •í•˜ì„¸ìš”.
- wandbë¥¼ ì‚¬ìš© ì¤‘ì´ë©´ run name/projectë„ í•¨ê»˜ ì •ë¦¬í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

---

<a id="contact"></a>
# 12. ğŸ“¬ ë¬¸ì˜

ì½”ë“œ êµ¬ì¡°, í•™ìŠµ ë°©ì‹, ì‹¤í—˜ ì„¤ì • ê´€ë ¨ ë¬¸ì˜ëŠ” GitHub Issueë¡œ ë‚¨ê²¨ì£¼ì„¸ìš”.
